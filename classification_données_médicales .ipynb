{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spyglass.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNJZMaIngr3x"
      },
      "source": [
        "import pandas as pd\n",
        "from  sklearn import decomposition\n",
        "\n",
        "def load_data(file : str) -> pd.DataFrame:   \n",
        "  \"\"\"\n",
        "  Load the dataset\n",
        "  \"\"\"\n",
        "  na_val = [\"n/a\", \"na\", \"?\", \"NaN\"]\n",
        "  df = pd.read_csv(file, na_values = na_val)\n",
        "  return df\n",
        "\n",
        "def replace_na_values(df : pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Return the data with the na values filled by the mean of the column\n",
        "  \"\"\"\n",
        "  n = len(df.columns)\n",
        "  for i in range(n):\n",
        "    index = 0\n",
        "    while type(df[df.columns[i]][index]) == float:\n",
        "        index+=1\n",
        "    if type(df[df.columns[i]][index])!= str:# and df[df.columns[i]].dtype != object:\n",
        "        df[df.columns[i]]=df[df.columns[i]].fillna(df[df.columns[i]].mean())   \n",
        "    else:\n",
        "        df[df.columns[i]]=df[df.columns[i]].fillna(df[df.columns[i]].mode()[0])\n",
        "\n",
        "  df = df.applymap(lambda x: x.replace(\"\\t\", \"\") if type(x) == str else x)\n",
        "  df = df.applymap(lambda x: x.replace(\" \", \"\") if type(x) == str else x)\n",
        " \n",
        "  return df\n",
        "\n",
        "def categoric_to_one_hot(df: pd.DataFrame) -> pd.DataFrame: \n",
        "  \"\"\"\n",
        "  Transform categorical data to one hot encoded vectors\n",
        "  \"\"\"\n",
        "  a=0\n",
        "  if df[df.columns[-1]].dtype!=int:\n",
        "    a=1\n",
        "  n = len(df.columns)\n",
        "  cols = [] #Categorical columns we will drop at the end\n",
        "  for i in range(n):\n",
        "    #print(df[df.columns[i]].dtype)\n",
        "    if df[df.columns[i]].dtype == str or df[df.columns[i]].dtype == object:\n",
        "      cols.append(i)\n",
        "      df[df.columns[i]] = pd.Categorical(df[df.columns[i]])\n",
        "      #One hot encoded version of the categorical column#\n",
        "      new_cols = pd.get_dummies(df[df.columns[i]], prefix = list(df.columns)[i] )\n",
        "      df = pd.concat([df, new_cols] , axis=1, sort=False)\n",
        "  df.drop(df.columns[cols], axis=1, inplace=True)\n",
        "  if a==1:\n",
        "    df=df.drop(df.columns[-1],axis=1)\n",
        " \n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_data(file : str) -> pd.DataFrame: \n",
        "  \"\"\"\n",
        "  Preprocess the data\n",
        "  \"\"\"\n",
        "  df = load_data(file) \n",
        "  df = replace_na_values(df)\n",
        "  \n",
        "  \n",
        "  return df"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR13KzCbibZk"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def apply_pca(df: pd.DataFrame) -> pd.DataFrame: \n",
        "  \"\"\"\n",
        "  Reduce dimensions thanks to pca\n",
        "  \"\"\"\n",
        "  pca = decomposition.PCA()\n",
        "  df=pca.fit_transform(df)\n",
        "  # Selecting only the best components\n",
        "  max_variance = np.max(pca.explained_variance_ratio_)\n",
        "\n",
        "  pc_number = len([v for v in pca.explained_variance_ratio_ if v >= max_variance/10000])\n",
        "  df = df[:, :pc_number]\n",
        "  return pd.DataFrame(df)\n",
        "  \n",
        "\n",
        "\n",
        "def create_datasets(db):    \n",
        "    data=preprocess_data(db)\n",
        "    y= data[data.columns[-1]]\n",
        "    '''for i in range(len(y)):\n",
        "      if y[i]>=5:\n",
        "        y[i]=0\n",
        "      else:\n",
        "        y[i]=1'''\n",
        "   \n",
        "    X=data.drop(data.columns[-1],axis=1)\n",
        "    #X=apply_pca(X)\n",
        "    X=pd.DataFrame(X)\n",
        "    return X, y"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthQsuG8ii7j"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def create_tree(db, criterion, splitter):  \n",
        "   \n",
        "   \"\"\"\n",
        "   db is the database file path\n",
        "   criterion is the criterion used in the decision tree\n",
        "   splitter is the splitter used in the decision tree\n",
        "   \"\"\"\n",
        "\n",
        "   X, y = create_datasets(db)\n",
        "  \n",
        "   max = 0\n",
        "   NUM_SPLITS = 5\n",
        "   clf = tree.DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_depth=5)\n",
        "   # split the data\n",
        "   kf = KFold(n_splits=NUM_SPLITS, shuffle=True)\n",
        "   # Train, test and compute the metrics on each split\n",
        "   \n",
        "   for train_index, test_index in kf.split(X):\n",
        "      \n",
        "      X_train = X.iloc[train_index,:]\n",
        "      y_train = y[train_index]\n",
        "      X_test = X.iloc[test_index,:]\n",
        "      y_test = y[test_index]\n",
        "      clf.fit(X_train, y_train)\n",
        "      # Predictions on the test set\n",
        "      predictions = clf.predict(X_test)\n",
        "      # we should chose the best train and test set\n",
        "      if max < metrics.accuracy_score(y_test, predictions):\n",
        "         max = metrics.accuracy_score(y_test, predictions)\n",
        "         X_train_f = X.iloc[train_index,:]\n",
        "         y_train_f = y[train_index]\n",
        "         X_test_f = X.iloc[test_index,:]\n",
        "         y_test_f = y[test_index]\n",
        "          \n",
        "   predictions = clf.predict(X_test_f)\n",
        "   clf.fit(X_train_f, y_train_f)\n",
        "   recall = metrics.recall_score(y_test_f, predictions)\n",
        "   precision=metrics.precision_score(y_test_f, predictions)\n",
        "   accuracy=metrics.accuracy_score(y_test_f, predictions)\n",
        "   print('precision= {}, recall= {}'.format(precision, recall))\n",
        "   #print('accuracy= {}'.format(accuracy))\n",
        "\n",
        "   return clf, precision, recall\n",
        "   #return clf, accuracy"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsKyZg0VLbbo"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "def create_forest(db, criterion):  \r\n",
        "   \r\n",
        "   \"\"\"\r\n",
        "   db is the database file path\r\n",
        "   criterion is the criterion used in the decision tree\r\n",
        "   splitter is the splitter used in the decision tree\r\n",
        "   \"\"\"\r\n",
        "\r\n",
        "   X, y = create_datasets(db)\r\n",
        "  \r\n",
        "   max = 0\r\n",
        "   NUM_SPLITS = 5\r\n",
        "   clf = RandomForestClassifier(criterion=criterion, max_depth=5)\r\n",
        "   # split the data\r\n",
        "   kf = KFold(n_splits=NUM_SPLITS, shuffle=True)\r\n",
        "   # Train, test and compute the metrics on each split\r\n",
        "   \r\n",
        "   for train_index, test_index in kf.split(X):\r\n",
        "      \r\n",
        "      X_train = X.iloc[train_index,:]\r\n",
        "      y_train = y[train_index]\r\n",
        "      X_test = X.iloc[test_index,:]\r\n",
        "      y_test = y[test_index]\r\n",
        "      clf.fit(X_train, y_train)\r\n",
        "      # Predictions on the test set\r\n",
        "      predictions = clf.predict(X_test)\r\n",
        "      # we should chose the best train and test set\r\n",
        "      if max < metrics.accuracy_score(y_test, predictions):\r\n",
        "         max = metrics.accuracy_score(y_test, predictions)\r\n",
        "         X_train_f = X.iloc[train_index,:]\r\n",
        "         y_train_f = y[train_index]\r\n",
        "         X_test_f = X.iloc[test_index,:]\r\n",
        "         y_test_f = y[test_index]\r\n",
        "          \r\n",
        "   predictions = clf.predict(X_test_f)\r\n",
        "   clf.fit(X_train_f, y_train_f)\r\n",
        "   #recall = metrics.recall_score(y_test_f, predictions)\r\n",
        "   #precision=metrics.precision_score(y_test_f, predictions)\r\n",
        "   accuracy=metrics.accuracy_score(y_test_f, predictions)\r\n",
        "   #print('precision= {}, recall= {}'.format(precision, recall))\r\n",
        "   print('accuracy= {}'.format(accuracy))\r\n",
        "\r\n",
        "   return clf, accuracy"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJZ0VgmEgs9A"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\r\n",
        "\r\n",
        "def create_gbc(db, criterion):  \r\n",
        "   \r\n",
        "   \"\"\"\r\n",
        "   db is the database file path\r\n",
        "   criterion is the criterion used in the decision tree\r\n",
        "   splitter is the splitter used in the decision tree\r\n",
        "   \"\"\"\r\n",
        "\r\n",
        "   X, y = create_datasets(db)\r\n",
        "  \r\n",
        "   max = 0\r\n",
        "   NUM_SPLITS = 5\r\n",
        "   clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\r\n",
        "   # split the data\r\n",
        "   kf = KFold(n_splits=NUM_SPLITS, shuffle=True)\r\n",
        "   # Train, test and compute the metrics on each split\r\n",
        "   \r\n",
        "   for train_index, test_index in kf.split(X):\r\n",
        "      \r\n",
        "      X_train = X.iloc[train_index,:]\r\n",
        "      y_train = y[train_index]\r\n",
        "      X_test = X.iloc[test_index,:]\r\n",
        "      y_test = y[test_index]\r\n",
        "      clf.fit(X_train, y_train)\r\n",
        "      # Predictions on the test set\r\n",
        "      predictions = clf.predict(X_test)\r\n",
        "      # we should chose the best train and test set\r\n",
        "      if max < metrics.accuracy_score(y_test, predictions):\r\n",
        "         max = metrics.accuracy_score(y_test, predictions)\r\n",
        "         X_train_f = X.iloc[train_index,:]\r\n",
        "         y_train_f = y[train_index]\r\n",
        "         X_test_f = X.iloc[test_index,:]\r\n",
        "         y_test_f = y[test_index]\r\n",
        "          \r\n",
        "   predictions = clf.predict(X_test_f)\r\n",
        "   clf.fit(X_train_f, y_train_f)\r\n",
        "   #recall = metrics.recall_score(y_test_f, predictions)\r\n",
        "   #precision=metrics.precision_score(y_test_f, predictions)\r\n",
        "   accuracy=metrics.accuracy_score(y_test_f, predictions)\r\n",
        "   #print('precision= {}, recall= {}'.format(precision, recall))\r\n",
        "   print('accuracy= {}'.format(accuracy))\r\n",
        "\r\n",
        "   return clf, accuracy"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDPW31F1AyTy"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def create_logistic_regression(db):\n",
        "   X, y = create_datasets(db)\n",
        "\n",
        "   clf = LogisticRegression(random_state=0)\n",
        "  # split the data\n",
        "   NUM_SPLITS = 5\n",
        "   kf = KFold(n_splits=NUM_SPLITS, shuffle=True)\n",
        "   # Train, test and compute the metrics on each split\n",
        "   max=0\n",
        "   for train_index, test_index in kf.split(X):\n",
        "      \n",
        "      X_train = X.iloc[train_index,:]\n",
        "      y_train = y[train_index]\n",
        "      X_test = X.iloc[test_index,:]\n",
        "      y_test = y[test_index]\n",
        "      clf.fit(X_train, y_train)\n",
        "      # Predictions on the test set\n",
        "      predictions = clf.predict(X_test)\n",
        "      if max < metrics.accuracy_score(y_test, predictions):\n",
        "         max = metrics.accuracy_score(y_test, predictions)\n",
        "         X_train_f = X.iloc[train_index,:]\n",
        "         y_train_f = y[train_index]\n",
        "         X_test_f = X.iloc[test_index,:]\n",
        "         y_test_f = y[test_index]\n",
        "          \n",
        "   predictions = clf.predict(X_test_f)\n",
        "   clf.fit(X_train_f, y_train_f)\n",
        "   recall = metrics.recall_score(y_test_f, predictions)\n",
        "   precision=metrics.precision_score(y_test_f, predictions)\n",
        "   accuracy=metrics.accuracy_score(y_test_f, predictions)\n",
        "   #print('precision= {}, recall= {}'.format(precision, recall))\n",
        "   print('accuracy= {}'.format(accuracy))\n",
        "\n",
        "   \n",
        "   "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH1-ZVgcjk4U"
      },
      "source": [
        "import graphviz\n",
        "def main(db):        \n",
        "  \"\"\"\n",
        "  db is the the file path\n",
        "  \"\"\"\n",
        "  #clf,precision,recall=create_tree(db, 'gini', 'random')\n",
        "  clf, accuracy=create_gbc(db, 'gini')\n",
        "  '''dot_data = tree.export_graphviz(clf,filled=True)\n",
        "  graph = graphviz.Source(dot_data)\n",
        "  graph.render('cancer_1')\n",
        "  tree.plot_tree(clf)'''\n",
        "  \n",
        "  "
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mELzfJZn_TnL",
        "outputId": "fad33b36-bcb6-4948-c03a-d285797f4e1b"
      },
      "source": [
        "\r\n",
        "main('/content/drive/MyDrive/SPYGLASS/data/annotations/medical_data3.csv')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy= 0.5263157894736842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTNSDKIROlUH"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    }
  ]
}